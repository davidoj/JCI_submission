%!TEX root = main.tex


\documentclass[USenglish,onecolumn]{article}
\usepackage[big]{dgruyter}

% \usepackage[utf8]{inputenc}%(only for the pdftex engine)
% \RequirePackage[no-math]{fontspec}%(only for the luatex or the xetex engine)
 
\input{Custom_Settings}

  
\begin{document}
 

  \articletype{Research Article{\hfill}Open Access}

  \author*[1]{David Johnston}

  \author[2]{Cheng Soon Ong}

  \author[3]{Robert C. Williamson}

  \affil[1]{Australian National University; E-mail: davidoj@fastmail.com.au}

  \affil[2]{Data61; E-mail; chengsoon.ong@anu.edu.au}

  \affil[3]{Universität Tübingen; Email: bob.williamson@uni-tuebingen.de}

  \title{\huge Causal Inference Without Interventions}

  \runningtitle{Inference Without Interventions}

  %\subtitle{...}

  \begin{abstract}
{Valid causal inferences from diverse data are a tantalizing but elusive prospect. Assumptions are necessary to make any inferences, and these assumptions cannot all be washed out by sufficiently large datasets, and they consequently require strong prior support. We argue that structural interventions in particular frequently embody assumptions not entailed by prior knowledge or by the given data. A key role of structural interventions is to identify fixed relationships that are shared by data arising from observation and by data arising from the consequences of a decision. We show that structural interventions are not required to analyse assumptions of fixed relationships like this. Our starting point is is to ground causal models in the problem of making good decisions, and our first result is an equivalence between decision models with "sequences of identical conditionals" and decision models that feature a symmetry we call "IO contractibility". This can be seen as a generalisation of De Finetti's work on exchangeability, itself an attempt to justify the conventional but seemingly mysterious assumption of sequences featuring a "shared but unknown probability distribution". We argue that IO contractibility is often an unreasonable assumption. Our second result is that IO contractibility may be implied by a combination of an observed conditional independence and a weaker assumption based on the principle of ``precedent'', which requires that everything that might be done has already been done before in some way and has been seen to work. We discuss a connection between this latter result and constraint based causal discovery.}
\end{abstract}
  \keywords{causal inference, decision theory}
   \classification[MSC]{62C99} % Decision theory - other (62C05 also maybe?)
 % \communicated{...}
 % \dedication{...}

  \journalname{Journal of Causal Inference}
\DOI{DOI}
  \startpage{1}
  \received{..}
  \revised{..}
  \accepted{..}

  \journalyear{2022}
  \journalvolume{1}
%  \journalissue{1}
 

\maketitle
\section{Introduction}

Judea Pearl's causal hierarchy distinguishes between three types of problems: prediction problems, intervention problems, and counterfactual problems \citep{pearl_book_2018}. Modelling an intervention problem requires a different kind of knowledge than that required for modelling predictions, and modelling a counterfactual problem requires a different kind of knowledge than that needed for modelling intervention problems.

While we think that Pearl's hierarchy offers an important insight into the differences between causal inference and classical statistics, we feel the terminology of interventions is confusing. In Pearl's theory, \emph{structural interventions} are used to model what he describes as intervention problems. Structural interventions are operations that transform a probability distribution according to a \emph{graphical causal model}. However, ``intervention problems'' refer to a broad class of problems that ask questions like ``what will happen if I do this? what will happen if I do something else instead?'' -- problems that we frequently encounter whether or not we make use of graphical causal models. This kind of problem often comes up we want to make a decision; given various options and some idea of the outcomes we would like to achieve, we want to know what the likely consequences of each option are.

As the terminology suggests, structural interventions and graphical causal models are often used as a tool for modelling the consequences of different options (that is, ``structural interventional models'' are used to model ``intervention problems'' broadly understood). However, using structural interventions to model the consequences of different options poses a conundrum. Our decision maker has some set of options $C$ to consider, and selecting an option $\alpha$ from this set is their decision problem. Given a dataset, they may attempt to infer causal relationships using causal discovery, or to estimate certain structural intervention-based effects of interest from the data along with whatever causal assumptions they supplied at the outset. Typically, whatever learning or estimation takes place, it does \emph{not} include an attempt to learn a correspondence between each of their options $\alpha$ and a structural intervention -- this correspondence is usually assumed to be prior knowledge. However, such a correspondence seemingly goes beyond the kind of prior knowledge a decision maker is likely to have.

The most common kind of structural intervention is known as a \emph{perfect} or \emph{hard} intervention. A perfect intervention is often denoted with the symbol $\mathrm{do}(\RV{X}=x)$. Given a probability distribution $\prob{P}$, a variable $\RV{X}$ and a causal graphical model $\mathcal{G}$ which, among other things, specifies a set of \emph{causal parents} $\mathrm{Pa}(\RV{X})$ of $\RV{X}$, the intervention $\mathrm{do}(\RV{X}=x)$ yields a new probability distribution $\prob{P}'$ such that the conditional probability $\prob{P}^{\prime \RV{X}|\mathrm{Pa}(\RV{X})}$ becomes the function $\cdot \mapsto \delta_x$, while all other conditional distributions of a ``child'' conditional on its ``parents'' according to $\mathcal{G}$ match their counterparts in $\prob{P}$ \citep[Sec. 1.3.1]{pearl_causality:_2009}

Thus identifying a perfect intervention $\mathrm{do}(\RV{X}=x)$ with an option $\alpha$ embodies a collection of assumptions -- first, it embodies the assumption that selecting the option $\alpha$ will force future observations of the variable $\RV{X}$ to take the value $x$. This information may sometimes (though not always) be available to decision makers. However, the identification of options with perfect interventions also embodies the assumption that selecting the option $\alpha$ will leave all ``parental conditionals'' with respect to $\mathcal{G}$ unchanged with the exception of $\prob{P}^{\prime \RV{X}|\mathrm{Pa}(\RV{X})}$. It's harder to see how a decision maker could know this.

It's possible that the decision maker might choose the graph $\mathcal{G}$ carefully just so that these additional conditions hold. If so, the decision maker must know exactly which conditionals are invariant a priori, and the graphical model $\mathcal{G}$ is merely a convenient shorthand for representing this knowledge. Typically a decision maker's prior knowledge will not be so extensive. Furthermore, adapting the graph to the set of options under consideration conflicts with normal practice in causal discovery where the learned graph does not depend on the options under consideration. Common measures of success in causal discovery are the structural intervention distance \citep{peters_structural_2015} and the structural hamming distance, neither of which depend on the options under consideration \citep{scherrer_learning_2022, toth_active_2022, brouillard_differentiable_2020, forre_constraint-based_2018, chickering_optimal_2003}. The idea that the relationships captured by a causal graph are independent of the options under consideration is also defended in \citet{pearl_does_2018}. 

% I don't know why these citations don't work
% \citep{ngGraphAutoencoderApproach2019 ,zhengDAGsNOTEARS2018, spirtes_causation_1993}

On the other hand, it is not obvious to us how a decision maker could know a priori that their options correspond to perfect interventions on some unknown ``objectively correct'' graphical model. We can also raise a concrete arguments against the notion that actions known in advance to affect a certain variable can typically be assumed to be interventions on that variable: there are multiple different ways to influence many variables we are interested in measuring and they cannot all be perfect interventions. \citet{hernan_does_2008,noauthor_does_2016} considers the example of different options that are known a priori to affect a person's body mass index, including diet plans, gastric bypass surgery and limb removal. These will all plausibly affect an individual's risk of death differently, and so they cannot all be modeled by the same intervention on body mass index. Further, it seems to us (as well as other authors in this exchange \citep{pearl_does_2018,hernanInvitedCommentaryCausal2009,shahar_association_2009}) that none of these options stand out as a strong candidate to be identified with a perfect intervention on body mass index. The difficulty is twofold: first, we don't know what makes the ``true graph'' true, and as a result it is not clear how to decide which option if any should be be identified with perfect interventions on this graph. Second, it is not even obvious which action represents the ``canonical'' way to alter a person's body mass index so it is not clear how we could verify that any procedure for learning a causal graph actually yields the correct interventions as a result.

Hernán argues that limb removal can be dismissed as an option because it is not interesting from a scientific or public health standpoint. While this is a reasonable contention, demanding that a causal graph correctly represent options that are interesting from a scientific or public health standpoint is a version of the strategy of of choosing the graph so that the options of interest are correctly represented, which as we've mentioned is not the standard practice in causal discovery.

Perhaps in recognition of the fact that many actions are not modeled by perfect interventions, there is a diverse array of structural interventions that can be found in the literature: a non-exhaustive review reveals perfect interventions or ``hard'' interventions \citep[ch. ~1]{pearl_causality:_2009,hauser_characterization_2012}, soft interventions \citep{correa_calculus_2020,eberhardt_interventions_2007}, general or fat-hand interventions \citep{eberhardt_interventions_2007,yang_characterizing_2018,glymour_evaluating_2017} and general interventions with unknown targets \citep{brouillard_differentiable_2020}. However, as in the case of perfect interventions, none of these generalised families of structural interventions make provisions for learning the option-intervention correspondence.

Notwithstanding our criticisms, the assumptions of invariant parental conditional distributions made by structural interventional models play a very important role in the interpretation of graphical causal models. If a decision maker wants to learn from some observed data so as to make a better decision, then these assumptions tell them what the observational data and the consequences of their decisions will have in common: the parental conditional distributions. In this paper, we show that structural interventions are not the \emph{only} way of analysing assumptions of this type.

We investigate whether certain assumptions of symmetry are a viable alternative to structured interventions for this task of using features of observations to forecast consequences of different options. In particular, we are interested in assumptions about the future being like the past. In probability models, an assumption of this type is the assumption of \emph{exchangeability}, which holds that reordering a sequence of observed variables leaves a forecaster with exactly the same prediction problem. This assumption cannot be applied as-is to decision models because, in decision problems, future events are affected by the choices made by the decision maker and therefore cannot be in all respects similar to previously observed events.

We investigate two different assumptions inspired by the idea that ``the future is in some sense like the past''. First, we introduce the idea of \emph{input-output contractibility} (IO contractibility), which can be viewed as a generalisation of exchangeability to decision models. In particular, we show a theorem analogous to De Finetti's famous representation \citep{de_finetti_foresight_1992} theorem holds; IO contractibility is equivalent to the assumption that there is a shared but unknown input-output map for both the observations and the consequences of a decision maker's choices (Theorem \ref{th:ciid_rep_kernel}). Unlike exchangeability, however, IO contractibility is not an appealing assumption in many data-driven decision problems.

We subsequently explore the general principle of \emph{precedent}, which holds that everything a decision maker can do has been done before.  We show that under the assumption of \emph{diverse precedent}, based on the principle of precedent, conditional independences imply the stronger conclusion of IO contractibility and with it the possibility of estimating an input-output conditional distribution from the given observational data (Theorem \ref{th:latent_to_observable}). A key assumption of Theorem \ref{th:latent_to_observable} is that the posterior of the parametrisation of a certain conditional distribution is dominated by the uniform measure. We discuss, speculatively, how this assumption may be related to causal structures, noting similar assumptions that appear in \citet{meek_strong_1995} and \citet{janzingCausalVersionsMaximum2021}.

Section \ref{sec:tech_prereq} introduces the formalism of decision models. These differ from probability models in that they are a map from a set of options to distributions over consequences. We make use of the notion of \emph{extended conditional independence} introduced by \citet{constantinou_extended_2017}, which is a notion of conditional independence relevant to decision models. Section \ref{sec:evaluating_decisions} introduces the idea of shared conditionally independent and identical responses, and shows that this is equivalent to the assumption of input-output in Theorem \ref{th:ciid_rep_kernel}. Section \ref{sec:precedent} explains the assumption of precedent and proves Theorem \ref{th:latent_to_observable} and discusses the interpretation of this assumption and its connection to structural causal models.

\subsection{Previous work on symmetries in causal inference}\label{sec:prev_work}

The approach that we take assumes that decision making is the fundamental problem that requires causal inference. This assumption motivates the formalism of ``decision models'' that we use to investigate the questions raised here. The broad idea of starting with the options available to a decision maker rather than starting with some foundational notion of causation is often called the \emph{decision theoretic approach to causal inference} \citep{heckerman_decision-theoretic_1995,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. \citet{lattimore_causal_2019,lattimore_replacing_2019} also document an approach to causal modelling that demands explicit consideration of the set of interventions, and is arguably an example of the decision theoretic approach.

\citet{lindley_role_1981} discussed sequences of exchangeable observations along with ``one more observation''. Lindley mentioned the application of this model to questions of causation, but did not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. \citet{rubin_causal_2005, imbens_causal_2015} made use of the assumption of models with exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, used graphical causal models to propose \emph{conditional exchangeability}, defined as the exchangeability of the non-intervened causal parents of a target variable under intervention on its remaining parents. Sareela et. al. suggested that this could be interpreted as a symmetry of an experiment involving administering treatments to patients with respect to exchanging the patients in the experiment. \citet{hernan_estimating_2006,hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss similar experimental symmetries. These symmetries are reminiscent of \emph{exchange commutativity} discussed here. They're not identical, however -- exchange commutativity can be justified by the equivalence of certain prediction problems that arise from a single experiment, instead of an equivalence of different experiments that arise from, for example, interchanging experimental subjects.

A different kind of regularity of causal models is given by the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020} and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}. This regularity is similar to the condition of \emph{locality} introduced here.

Theorem \ref{th:latent_to_observable} was inspired by the idea of \emph{causal inference by invariant prediction} \citep{peters_causal_2016}. While both the assumptions and the conclusions drawn in that work differ from the assumptions and conclusion of Theorem \ref{th:latent_to_observable}, both proceed from an idea that can be roughly described as ``things I can do have been done before'' and both look for variable pairs $\RV{X}$ and $\RV{Y}$ such that the distribution of $\RV{Y}$ given $\RV{X}$ doesn't change after actions are taken. Finally, the variable described in that work as ``the environment'' is similar to the variable $\RV{Z}$ in Theorem \ref{th:latent_to_observable} in that neither variable needs to be IID, and both variables are only necessarily of interest in the observation set, and need not be of any interest for the consequences of actions.

\input{technical}
\input{contractibility}
\input{precedent}

\bibliographystyle{plainnat}

\bibliography{library}

\appendix

\input{appendix}

\end{document}