
%!TEX root = ../main.tex

\chapter{Introduction}\label{ch:introduction}

\section{Making decisions with data}

Beginning in the 1930s, a number of associations between cigarette smoking and lung cancer were established: on a population level, lung cancer rates rose rapidly alongside the prevalence of cigarette smoking. Lung cancer patients were far more likely to have a smoking history than demographically similar individuals without cancer and smokers were around 40 times as likely as demographically similar non-smokers to go on to develop lung cancer. In laboratory experiments, cells which were introduced to tobacco smoke developed ciliastasis (a slowing or stopping of the beating of cilia in the upper respiratory tract), and mice exposed to cigarette smoke tars developed tumors \citep{proctor_history_2012}. Nevertheless, until the late 1950s, substantial controversy persisted over the question of whether the available data was sufficient to establish that smoking cigarettes \emph{caused} lung cancer. Cigarette manufacturers famously argued against any possible connection \citep{oreskes_merchants_2011} and Ronald Fisher in particular argued that the available data was not enough to establish that smoking actually caused lung cancer \citep{fisher_cancer_1958}. Today, it is widely accepted that cigarettes do cause lung cancer, along with other serious conditions such as vascular disease and chronic respiratory disease \citep{world_health_organisation_tobacco_nodate,wiblin_why_2016}.

The question of a causal link between smoking and cancer is a very important one to many different people. Individuals who enjoy smoking (or think they might) may wish to avoid smoking if cigarettes pose a severe health risk, so they are interested in knowing whether or not it is so. Additionally, some may desire reassurance that their habit is not too risky, whether or not this is true. Potential and actual investors in cigarette manufacturers may see health concerns as a barrier to adoption, and also may personally want to avoid supporting products that harm many people. Like smokers, such people might have some interest in knowing the truth of this question, and a separate interest in hearing that cigarettes are not too risky, whether or not this is true. Governments and organisations with a responsibility for public health may see themselves as having responsibility to discourage smoking as much as possible if smoking is severely detrimental to health. The costs and benefits of poor decisions about smoking are large: 8 million annual deaths are attributed to cigarette-caused cancer and vascular disease in 2018 \citep{world_health_organisation_tobacco_nodate} while  global cigarette sales were estimated at US\$711 billion in 2020 \citep{noauthor_cigarettes_nodate} (a figure which might be substantially larger if cigarettes were not widely believed to be harmful).

The question of whether or not cigarette smoking causes cancer illustrates two key facts about causal questions: first, having the right answers to causal questions can underpin decisions of tremendous importance to large numbers of people. Second, confusion over causal questions can persist even when a great deal of data and and a great many facts relevant to the question are agreed upon. Understanding how the world might be influenced is often both valuable and difficult.

There are a number of different ways one could go about learning how to influence the world from data. One option is to try to obtain data that we're confident tells us directly about the consequences of the different options under consideration. For some purposes, data produced from well-conducted experiments is widely agreed to provide reliable information about the effectiveness and safety of treatments tested in the experiment. 

Alternatively, we could use the data to solve an intermediate problem, and make use of pre-existing knowledge about how to influence the world given a solution to this problem. For example, if I am on a long car trip, my tank is three quarters empty and I'm at the last fuel station for 200km, then given an answer to the question of how far my car will travel on one quarter a tank of fuel it is easy to decide whether or not I should fill up right now, and if I've logged my mileage previously I might use the data I collected to answer this question. In this example, the data don't tell me directly whether or not I should fill up. 

However, we might be in a position where, we aren't so confident that the data we can acquire can provide us with reliable guidance directly about our choice, and we don't know of any surrogate problems that make the causal question straightforward. In this case, we may want to make use of a formal theory of causal inference. When we can't see the solution immediately, a theory of causal inference can help us see more clearly the consequences of things we already know. It can also provide a language that we can use to discuss assumptions and conclusions with other people.

A lot can be said for the first two options. ``Collecting the right data'' has driven some of the most significant recent developments in algorithmic decision making. Operational advances that enable controlled experiments to be conducted at large scales have driven substantial changes in of many online businesses \citep{kohavi_surprising_2017}, and Abhijit Banerjee and Esther Duflo were recently awarded a Nobel prize in part for their pioneering role in the use of large numbers of randomised controlled trials (RCTs) to assess the effectiveness of many different development interventions in many different contexts \citep{zhang_abdul_2014}. Some fields of science have also been significantly affected by ``negative progress'' in the science of assessing experimental results. For example, in psychology in particular, replication attempts have shown that causal conclusions from experimental psychological data are less robust than many had hoped or (perhaps) believed \citep{open_science_collaboration_estimating_2015,stroebe_what_2019}. At the same time, standards for what constitutes a ``well-conducted'' experiment have risen across many fields \citep{nosek_preregistration_2018,liberati_prisma_2009}.

``Solving intermediate problems'' has also been behind tremendous technological advances. A machine that recognises your face has no useful impact in the world by itself, but there are a lot of people who know how to use such a machine for commercial or other purposes.

While a lot of progress can be made by getting around the problem that it's hard to make data-driven decisions that directly aim to influence the world, we think it is also interesting to attack the problem directly. One question we might want to ask is: \emph{why} is this problem hard? While we don't claim to know for sure, a major source of difficulty seems to come from the fact that decision making requires us to consider hypotheticals.

\subsection[Hypotheticals]{Decision making requires thinking about hypotheticals}\label{sec:assumptions}

Data driven prediction problems and data driven decision making problems can have a lot in common. The outcomes some people are interested in predicting are often outcomes other people want to influence. A forecaster might want to predict the winner of the next election, while a party strategist is interested in maximising their party's chance of victory. A product manager may be simultaneously interested in accurately inferring the sentiment expressed in reviews of their product, and in making product changes that increase the frequency that this sentiment is positive. Furthermore, data relevant to prediction is often relevant to decision making and vise-versa. Political parties often reason that electorates in which their predicted chance of victory is very low are not worth investing campaign resources in, and if a forecaster learns of evidence that one party had adopted a particularly effective election strategy they might want to revisit their prediction of the eventual election winner. The overlap is not perfect: comprehensive electorate level polls are probably more useful to the forecaster while small-scale controlled experiments are probably more useful to the strategist, but there's a lot of overlap nonetheless.

A distinguishing feature of decision making problems is that they demand the decision maker consider a collection of hypotheticals, most of which are never realised. The strategist must consider a number of different strategies to pursue, and ultimately only learns of the outcome under the strategy their party \emph{did} pursue. The election forecaster, on the other hand, can consider a number of different forecasts, but after the fact they learn exactly how accurate \emph{every} forecast was and, in particular, whether the forecast they made was better than others they could have made.

Statistical probability is well-established and widely used as a formal theory of data-driven prediction. We can speculate, on the basis of the observations above, that a formal theory of data-driven decision making may be obtained by augmenting statistical probability with the right kind of hypotheticals. In fact, even though causal inference is not quite synonymous with data-driven decision making, the most widely used theories of causal inference are theories of statistical probability augmented with a particular notion of hypothetical.

To understand the role of hypotheticals in theories of causal inference and decision making, it helps to think about the role of random variables in statistical probability. Random variables have two ``faces''; on one hand, they are defined as measurable functions whose domain is the sample space, and this allows us to reason about them using the tools of mathematics. However, they \emph{also} refer to the results of measurements conducted in the real world. This feature of random variables is not a consequence of any mathematical definition, but it is this connection to the real world that allows us to use insights derived from mathematical reasoning to inform predictions we can make about real-world events.

Hypotheticals are similarly two-faced. On the one hand, particular kinds of hypotheticals have formal definitions in theories of causal inference and decision making, and on the other hand the fact that they point to something outside the mathematical model is what allows us to use the model to help us make decisions (or to do whatever else we might want to do with a causal model). Just what it is that hypotheticals in causal models refer to is a trickier question than what random variables refer to.

\subsection{Structural interventions}

One widely-used theory of causal inference uses the term \emph{interventions} for the relevant class of hypotheticals. Interventions are defined with respect to a particular set of variables that we will call \emph{causal variables}. A graphical causal model (for the purposes of this introduction) assigns to each causal variable a possibly empty set of \emph{parents} (or causes) selected from the rest of the causal variables. Usually this assignment of parents has no cycles, but there are versions of interventional models that allow cycles \citep{bongers_theoretical_2016,forre_markov_2017}.

In this manner, the assignment of parents can be represented with a directed acyclic graph -- each causal variable is associated with a node of the graph, and an arrow $\RV{X}\rightarrow \RV{Y}$ appears in the graph just when $\RV{X}$ is a parent of $\RV{Y}$. For example, the following graph
\begin{align}
\tikzfig{simple_dag}
\end{align}
identifies three causal variables $\RV{X}$, $\RV{Y}$ and $\RV{Z}$, and identifies $\RV{X}$ and $\RV{Z}$ as parents of $\RV{Y}$, $\RV{Z}$ as a parent of $\RV{X}$ and $\RV{Y}$ as a parent of $\RV{X}$.

Given a graphical causal model and a joint probability distribution over the causal variables, an intervention on a causal variable $\RV{X}$ is formally an operation that alters the distribution of $\RV{X}$ conditional on its parents in a known way, while not affecting the distribution of any other causal variable conditional on \emph{its} parents.

Beyond the formalism of structural causal models, \emph{interventions} on $\RV{X}$ are usually taken to refer to actions that can be taken that alter the real thing represented by $\RV{X}$ in a predictable way while also avoiding directly influencing anything else (or at least, any other causal variable that appears in the model). While this sounds complicated, it can sometimes be reasonably clear what it means for an action to avoid influencing other causal variables; for example, take $\RV{Z}$ to be the last month's average rainfall, $\RV{X}$ to be the average number of flowers I see when I walk for the past month and $\RV{Y}$ to be how often I walk in the past month. In a common sense way, deciding to go for a walk today changes $\RV{Y}$ but has no effect on rainfall or flower growth. Also in a common sense way, someone else planting flowers might induce me to walk more often, but does not affect the rainfall or my inclination to walk holding weather and flowers constant. Finally, seeding clouds might cause more rainfall, which could cause more flowers and might affect my walking in an unpredictable way, but it probably doesn't alter the dependence of my walking on the weather and the scenery together. 

However, it's not always clear how to interpret interventions. A particular example that has received extended discussion is the idea of an intervention on ``obesity''. \citet{hernan_does_2008,noauthor_does_2016} have argued that ``intervention'' on obesity -- as measured by a patient's body mass index, or their weight divided by the square of their height -- are ill-defined. They note that several different actions might alter a person's body mass index, such as diet, exercise or gastric bypass surgery, and it isn't clear which of these if any count as an intervention. \citet{pearl_does_2018} responded that an intervention could be defined as an ideal action setting body mass index ``performed by nature'', although we find this idea hard to understand. 

\subsection{Potential outcomes}

The other widely used theory of causal inference uses the term \emph{potential outcomes} for its class of hypotheticals. Formally, potential outcomes are statistical random variables associated with a pair of ``ordinary'' statistical random variables. For example, if we have $\RV{X}$ again representing the mean number of flowers I see every walk and $\RV{Y}$ again representing my frequency of walks, we can define a vector of potential outcomes as a copy of $\RV{Y}$ for every possible value of $\RV{X}$: $(\RV{Y}^x)_{x\in [0,100]}$ ($\RV{X}$ is real-valued because it is a mean). 

Beyond the formalism  of the model, a potential outcome $\RV{Y}^{50}$ represents ``how often I would've taken walks last month if I had seen 50 flowers each time on average''. In cases like this, it's not clear that there is a common-sense interpretation of potential outcomes. Counterfactual statements themselves are often difficult enough to grasp that some additional theory seems needed to make sense of them.

There are some cases where potential outcomes have a somewhat easier interpretation. A version of a classic example is when $\RV{X}$ represents whether or not a patient took antibiotics and $\RV{Y}$ represents the presence of an ear infection at a follow-up appointment. In this case $\RV{Y}^0$ represents the presence of an ear infection ``had the patient not taken antibiotics'' and $\RV{Y}^1$ represents the presence of an ear infection ``had the patient taken antibiotics''. If we adopt the patient's point of view, we can view these as the consequences that the patient should consider when deciding whether or not to take the medicine.

In fact, some authors have argued that potential outcomes are underpinned by choices -- that is, we have potential outcomes $\RV{Y}^X$ precisely when $X$ is a set of options that somebody could, in principle, have chosen \citep[~pg. 4]{imbens_causal_2015}. 

In the philosophical investigation of the interpretation of counterfactuals, accounts based on structural interventions are one of the most prominent theories \citep[Section 3.3]{starr_counterfactuals_2021}, though there are several versions of this account and like all of the other theories of counterfactuals they are controversial.

\subsection{Successes of theories of causal inference}

Formal theories of causal inference exist to help us to draw reliable conclusions from data when informal reasoning isn't good enough to do so. A full account of the successes of intervention and potential outcomes based theories is beyond the scope of this introduction, but a brief overview will help to situate the work in this thesis in the broader context of causal inference theories.

\subsubsection{Successes of potential outcomes}

Potential outcomes models are characterised by the inclusion of potential outcomes variables, typically notated with superscripts $\RV{Y}^0$, $\RV{Y}^1$. These variables represent counterfactual notions -- $\RV{Y}^0$ can be read ``the value that $\RV{Y}$ would have taken, had $\RV{X}$ been 0''. The potential outcomes framework has been particularly influential in econometrics, with use of potential outcomes in that field predating the actual term ``potential outcomes''. In econometrics, and in other areas that potential outcomes have been widely used, models often involve people acting deliberately (or, sometimes, rationally) and associate potential outcomes with prospective consequences of people's choices.

\textbf{Models involving rational agents}: As we have noted, in some situations the potential outcomes $\RV{Y}^0$, $\RV{Y}^1$ and so forth look like a set of prospective consequences that a decision maker is choosing between. In some settings, we can let them be precisely that - a decision maker's expectation of the consequences of different actions they can choose. An early application of potential outcomes was to the problem of determining supply and demand curves given data on the quantity of goods exchanged and the price at which they were exchanged \citep{tinbergen1997determination,haavelmo_statistical_1943}. In this early work, a ``supply curve'' is defined as a function that maps a hypothetical price to the quantity of goods that would be supplied, were that the actual price of goods, which is for all intents and purposes a vector of potential outcomes $(\RV{Y}^x)_{x\in A}$ for some set $A$ of prices under consideration. 

Note that, in this case, ``what potential outcomes mean'' might be able to be grounded in a theory of behaviour of buyers and sellers. We can suppose that sellers are actually asking questions like ``how much would I sell if I asked for a price $x$?'' and getting hints about the answer from buyers and other sellers. Under some idealisations -- for example, maybe we require the buyers and sellers to all agree on questions of this nature -- we might be able to consider the potential outcomes to represent the the traders' answers to these questions.

\textbf{Analysis of randomised experiments:} The potential outcomes framework offers an account of what it is that a randomised experiment achieves so that it enables causal conclusions to be drawn. Under this framework, the critical condition is the statistical independence of the input $\RV{X}$ from the vector of potential outcomes $(\RV{Y}^x)_{x\in X}$ (in the potential outcomes framework, $\RV{X}$ is often called an \emph{assignment}). If the value of $\RV{X}$ is completely determined by some physical randomisation procedure then (so the argument goes) it must be independent of the potential outcomes. 

A more general kind of randomised experiment completely determines the values of inputs based on a collection of other variables $\RV{W}$ called \emph{covariates} and some physical randomisation procedure. In this kind of experiment, the inputs are independent of the potential outcomes conditional on the covariates. When this holds, the inputs $\RV{X}$ depend probabilistically on the covariates $\RV{W}$, and this dependence is usually called the \emph{assignment mechanism}. When this dependence is known or able to be estimated, it can facilitate the calculation of many causal effects of interest (see Section \ref{sec:potential_outcomes} for more details). Using the potential outcomes framework, many techniques have been developed to estimate the assignment mechanism and to estimate causal quantities of interest given the assignment mechanism -- for example, many methods and worked examples can be found in \citet{imbens_causal_2015}.

Because the potential outcomes framework doesn't offer a theory of counterfactuals, it doesn't come with an explanation of why the inputs are independent of the potential outcomes in a randomised experiment -- this is a matter that stands outside the formal theory. One could consider an explanation that goes something like: we imagine the potential outcomes to be fixed at the time that the inputs are decided, so the inputs have no influence over them. Furthermore, if the inputs are physically randomised, the potential outcomes can have no influence over the inputs, and so the two are independent. We might also consider the converse of this claim: perhaps any reasonable theory of counterfactuals must yield the conclusion that potential outcomes are (statistically) independent of any physically randomised inputs.

Unlike the case of supply and demand, the deliberate action in a randomised experiment is the assignment carried out by the experimenter rather than the choices made by subjects of the model.

\textbf{Subpopulations with different behaviour:} Rather than having an input $\RV{X}$ that is completely determined by a physical randomization mechanism, sometimes experiments of interest have some physically randomized $\RV{Z}$ that influences but does not fully determine $\RV{X}$. For example, if $\RV{X}$ records whether or not someone took a medicine, $\RV{Z}$ might record whether or not the medicine was prescribed. Experimenters might be able to have prescriptions randomized, but not the actual act of taking the medicine. The potential outcomes framework first offers us a way to understand that this is not analogous to an experiment where $\RV{X}$ is randomised: by application of probability theory, the fact that the potential outcomes are independent of $\RV{Z}$ does not mean that they are independent of $\RV{X}$.

A notable result proven using the potential outcomes framework is that, under an assumption that prescribing a medicing never induces anyone to avoid the medicine who would otherwise have taken it (``no defiers''), it is possible to determine the effect of taking the medicine on the subpopulation of ``compliers'' -- people who were induced by the prescription to switch from not taking the medicine to taking it. See \citep{imbens_identification_1994} for more details.

In this setting, there are often deliberate choices carried out by the experimenter -- namely, the assignment of $\RV{Z}$ -- and also deliberate choices carried out by experimental subjects -- the choice of $\RV{X}$.

\subsubsection{Successes of structural interventional models}

Structural interventional models feature a collection of causal variables, and each variable is assigned a set of parents from the remaining causal variables. Each causal variable may be intervened on, which in general alters the distribution of the variable conditional on its parents while not changing the distribution of any other causal variable conditional on that variable's parents.

\textbf{A formal theory of directed causal relationships:} A point that is repeatedly made by \citet{pearl_causality:_2009} and \citet{pearl_book_2018} is that informal notions of causal relationships play a key role in the formulation of many statistical models. For example, in the account of randomised experiments above, we said that the treatment assignment was ``determined by'' a physically random procedure. This statement is not backed by a formal theory, but the relation invoked by the phrase ``determined by'' is something like a causal relationship. It implies that the treatment assignment and the output of the randomisation are deterministically related, but ``being determined by something'' is not a symmetric relationship.

Structural interventional models offer a theory of causal relationships that aims to clarify these intuitions. They have been used to analyse questions like ``what is the likelihood that the medicine caused the reaction?'' (\citet[ch. ~9]{pearl_causality:_2009}, \citet{pearl_causes_2015}), which differ from traditional causal inference questions that are more focused on the consequences of actions than on attributing responsibility. 

The question of whether the structural interventional account explains causal intuitions has been taken on by philosophers (see, for example, \citet{woodward_causation_2016}), but whether it is successful in doing so is contested \citep{cartwright_modularity_2001}.

Interventions might not be the \emph{only} possible way to ground intuitions about directed causal relationships. An alternative proposition is that the distribution of a cause and the distribution of an effect conditional on the distribution of the cause should be \emph{algorithmically independent}, or that the relation between them should be \emph{generic} \citep{lemeire_replacing_2013}. Because this principle can induce directed relationships between pairs of variables, it can potentially offer an account of directed causal relationships without appealing to interventions, though it has been substantially less studied than the structural intervention account of directed relationships. However, unlike the interventional theory, this does not seem to tell us how causal relationships should inform our ideas of the consequences of taking an action\footnote{Throughout this thesis, we use the term ``intervention'' to refer to an operation defined in the structural interventional account of causal inference, or the interpretation of this operation. We use the word ``action'' to refer to something that may or may not be interpretable as an intervention.}.

One of the contributions of the present work is Theorem \ref{th:det_obs_to_cons} in Section \ref{sec:precedent}, which shows that under an assumption of generic relationships between the conditional distributions of causes and effects together with the assumption that a proposed plan of action has a precedent then conditional independences in observed data can imply certain relationships do not change under any action the decision maker might take. Invariant relationships under action, which are taken to be axiomatic in the theory of structural interventions, can be shown to follow from the assumption of generic relationships between conditional distributions and the previously mentioned assumption that actions are precedented. We think this suggests that it may be possible to forge a unified view of directed causal relationships that subsumes both the notion that causal relationships should be invariant under action and the notion that conditional distributions should be generically related in the causal direction, although precisely how to do this is an open question.

\textbf{Causal inference under generic assumptions:} Traditionally, analysis of causal inference problems involves certain non-generic assumptions like the assumption of independence of inputs and potential outcomes. These assumptions are non-generic because they do not apply to arbitrary causal inference problems, and so the analysis made under these assumptions can only be applied to data generated in particular contexts (for example, in controlled experiments).

The structural models tradition, however, has fostered the analysis of \emph{causal discovery}, which is the problem of learning causal relationships from data which may not be known to satisfy certain strong assumptions. There are two main approaches to causal discovery: conditional independence-based causal discovery infers a family of causal graphs from conditional independences inferred from a dataset, while the previously mentioned theory that conditionally probabilities should be algorithmically independent in the causal direction has led to a wide variety of different approaches for discovering the direction of causation. Early examples of conditional independence based inference are the PC algorithm and the Causal Inference Algorithm \citep[Ch. 5 \& 6]{spirtes_causation_1993} and Greedy Equivalent Search \citep{chickering_optimal_2003,chickering_learning_2002}, while more recently it has been discovered that the problem of searching for a graph satisfying inferred conditional independences can be posed as a continuous optimization problem \citep{zheng_dags_2018,ng_graph_2019}. Examples of the applications of the idea of algorithmic independence include methods based on the assumption of additive noise \citep{hoyer_nonlinear_2009,shimizu_linear_2006} and so-called \emph{information geometric causal inference} (IGCI) methods \citep{daniusis_inferring_2012}. Reviews of these methods can be found in \citep[ch. 4, 5, 6 \& 7]{peters_elements_2017} and \citep{mooij_j.m._distinguishing_2016}.

While the aim of this analysis is to discover causal relationships from generic data, in practice the key assumptions are not completely generic. \citet{uhler_geometry_2013} examined how frequently the key assumption of $\lambda$\emph{-faithfulness} underpinning the conditional independence based approach is violated, and finds that (under their assumptions) models with more than 10 variables and relatively dense causal connections almost always violate the condition. Owing to the fact that algorithmic independence is incomputable and suitable approximations have to be found for practical algorithms, the algorithmic independence based approach has typically involved special conditions like linear causal relationships with non-Gaussian additive noise.

\textbf{Causal identification in complex models:} Potential outcomes approaches have proposed a wide variety of sufficient assumptions for estimating causal effects, but there are models in which causal effects can be estimated that are typically ignored by the potential outcomes approach. The graphical models community usually separates the problem of \emph{identification} and \emph{estimation}; a causal effect is \emph{identified} if it can be computed from the joint probability of the observed variables. If this is possible, then the causal effect could in principle be calculated from an estimate of the joint probability (though estimating the entire joint probability is usually more than is needed). 

One of the simplest graphical models in which causal effects are identified that haven't received much attention in the potential outcomes literature is the ``front-door'' condition. Given a graphical model for which it is impossible to identify the effect of $\RV{X}$ on $\RV{Y}$, but the effect of $\RV{X}$ on $\RV{W}$ and the effect of $\RV{W}$ on $\RV{Y}$ can be separately identified, then it is possible to compose the two to identify the effect of $\RV{X}$ on $\RV{Y}$ \citep[Section 3.3.2]{pearl_causality:_2009}. In fact, a complete characterisation of the identifiability of graphical models has been given by \citet{shpitser_complete_2008}, and a more recent alternative characterisation is presented in \citet{richardson_nested_2017}.   

\subsection{Challenges to popular theories of causal inference}

Despite the fact that both the potential outcomes and structural interventional approaches have substantially advanced everyone's understanding of causal inference, we believe that both approaches face difficulties that make them hard to apply outside of special settings. The difficulty with potential outcomes is easy to state: the potential outcomes framework offers no clear theory of counterfactuals. Thus the appropriate interpretation of counterfactuals statements must either be obvious or established by convention or else communication using the framework risks confusion. 

As we noted earlier, some authors have suggested that potential outcomes should represent the potential consequences of choices or actions, and that this might perhaps form the basis of a theory of counterfacutals. The work in this thesis builds a theory of causal inference starting from the view that the fundamental problem to be addressed is a problem of making informed choices. We don't have a particularly strong view on whether our theory is fundamentally different to potential outcomes, or whether the two approaches are fundamentally similar but look different because we focus on modelling an abstract problem of making informed choices, while potential outcomes is often focused much more on various concrete problems.

Alternatively, some authors have argued that the theory of structural interventions is the appropriate theory of counterfactuals for potential outcomes \citep[chap. ~7]{pearl_causality:_2009}. In this case, the following remarks on structural interventional theories apply.

\subsubsection{Difficulties for structural models}

If a decision maker has a sound informal understanding of causal relationships relevant to their problem, structural models are often an excellent tool to formalise this understanding and derive conclusions from it. However, if a decision maker is dealing with a problem where they do \emph{not} have a sound informal understanding of causal relationships, what does the structural approach offer them? The structural model community might suggest that they perform \emph{causal discovery}; this is some procedure that takes their data and offers them a best guess of the structural model associated with this data.

What role should this learned structural model play in the decision maker's subsequent deliberation? We propose three answers to this question:
\begin{enumerate}
    \item The structural model tells the decision maker what their options are and what consequences are likely to follow
    \item The structural model can be combined with the decision maker's prior knowledge of what their options are to offer an assessment of their consequences
    \item The structural model and the decision maker's options coevolve; perhaps the decision maker has an initial idea of what their options are which motivates a particular avenue of causal discovery which, in turn, migh prompt the decision maker to reevaluate their options and so forth
\end{enumerate}

We consider the second and third answers reasonable, though the third answer is beyond the scope of this thesis. However, these two answers seem to be in tension with typical practice in causal discovery. Causal discovery algorithms typically take only the given data as input, and depend in no way on any specification of the decision maker's options. Despite this, whether or not a structural model can offer an assessment of the consequences of a set of options is sensitive to the options under consideration.

\begin{example}[Different sets of options require different models]\label{ex:prob_int_2}
Suppose on day $i$, at some point during the day, a volunteer Ella looks at the current outside temperature and logs whether it is ``cold'' or ``hot'' as $\RV{T}_i$, whether or not she's wearing a jumper as $\RV{X}_i$ and whether she feels cold, comfortable or hot as $\RV{Y}_i$. Under normal circumstances, in cold weather she usually wears a jumper and feels comfortable, and in hot weather she wears no jumper and feels comfortable. If she is uncharacteristically not wearing a jumper on cold days, she feels cold, and if she is wearing one on hot days she feels hot.

Suppose she's asked to wear her jumper no matter what. In this case, she will feel comfortable on cold days as before, and will feel hot on hot days also as before. Under this ``intervention'', the relationship between her perceived body temperature and the joint specification of her clothing and the weather is unchanged. If we assume an acyclic structural model, that the instruction to wear a jumper should indeed be modeled by an intervention in this model, and that the temperature may be a cause but not an effect of body temperature and jumper wearing, then we can conclude from the results of the intervention that jumper wearing causes body temperature perception and that this relationship is unconfounded given the daily temperature:
\begin{align}
    \tikzfig{simple_dag_jumper}\label{ex:simple_dag_jumper}
\end{align}

Suppose we \emph{also} want to consider the effect of actions affecting Ella's perceived body temperature. We could ask her to exercise intensely before filling out the survey on some days. What we find is, because exercise raises her body temperature, after exercising she feels comfortable with no jumper in cold weather and feels hot in hot weather with no jumper. However, we \emph{also} find that she now prefers not to wear a jumper in cold weather. This finding does \emph{not} correspond to the structural model \eqref{ex:simple_dag_jumper}.

The following modification to this structure also does not yield the desired result:
\begin{align}
    \tikzfig{simple_dag_jumper_bd}
\end{align}
Under normal circumstances, the condition $\RV{Y}_i=\text{``feel hot''}$ always happened when Ella was wearing a jumper, but under the exercise condition it corresponds to no jumper wearing. Thus the conditional distribution of $\RV{X}_i$ given $\RV{T}_i$ and $\RV{Y}_i$ is not the same in the exercise condition to the normal condition.

If we introduce a new unobserved variable $\RV{Y}^X$ that represents Ella's beliefs about how she would feel if she were or were not to wear a jumper, then the following structural model can yield the desired result for both interventions (note: there are also other possibilities):
\begin{align}
    \tikzfig{not_simple_dag_jumper}\label{ex:not_simple_dag}
\end{align}

Here, we say that Ella's beliefs about her perceived body temperature are influenced by the outside temperature, whether or not she's wearing a jumper and her actual perceived body temperature. Here, once again, her perceived temperature is caused only by $\RV{T}_i$ and $\RV{X}_i$, which reproduces the fact that her perceived temperature depends on these variables in exactly the same way after intervention on her jumper wearing. However, the only cause of her jumper wearing is her beliefs about how comfortable she will feel with a jumper on, which can be influenced by interventions on her body temperature. Thus this model can also reproduce the assumed fact that when her body temperature is intervened on, she acts to maintain a comfortable equilibrium.

Concretely, taking $-1$ to be ``cold'', $0$ to be ``comfortable'', $1$ to be ``hot'', $0$ also for ``no jumper'' and $1$ for ``jumper'', we specify the model as
\begin{align}
    \RV{T}_i &\sim U(\{-1,0\})\\
    \RV{Y}^X_i &\leftarrowtriangle \begin{cases}
                                x\mapsto x-1 & \text{ if } \RV{Y}_i = \RV{X}_i -1\\
                                x\mapsto x & \text{ if } \RV{Y}_i = \RV{X}_i\\
                                x\mapsto 1 & \text{ if } \RV{Y}_i = \RV{X}_i+1
                            \end{cases}\\
    \RV{X}_i &\leftarrowtriangle \begin{cases}
                    1 &\text{ if }\RV{Y}^X_i=x\mapsto x-1\\
                    0 &\text{ otherwise}
    \end{cases}\label{eq:struct_x}\\
    \RV{Y}_i &\leftarrowtriangle \RV{T}_i + \RV{X}_i\label{eq:struct_y}
\end{align}
Here, a left arrow indicates a causal assignment. This has a unique solution for each value of $\RV{T}_i$. Instructing Ella to wear a jumper is modelled by replacing the right hand side of Eq. \eqref{eq:struct_x} with 1 and instructing Ella to exerciese is modelled by replacing the right hand side of  Eq. \eqref{eq:struct_y} with $\text{max}(1,\RV{T}_i + \RV{X}_i + 1)$. See \citet{bongers_theoretical_2016,forre_causal_2020} for a much more in-depth treatment of structural models with cycles, and see \citet{eberhardt_combining_2010,ghassami_causal_2020} for algorithms for discovering linear cyclic causal structures

Suppose, finally, we are interested in the results of a) tampering with the device Ella uses to determine the temperature each day and b) putting up a large structure to shade Ella's house. Both of these actions are likely to affect the reading $\RV{T}_i$, but otherwise have different consequences. It does not seem possible, therefore, that any single structural model limited to the variables $\RV{X}_i$, $\RV{Y}_i$ and $\RV{T}_i$ and functions thereof can represent the consequences of both of these actions.
\end{example}

In this example, different kinds of models are suitable for assessing the consequences of different sets of options, and a model accommodating all of the options considered is substantially more complex than a model accommodating only the request to wear a jumper. Maybe it is in principle possible to come up with a structural model that covers every set of options anyone might want to consider -- though it's far from obvious that this really is possible -- but it would be very surprising to us if there was any practical way to do so.

Practically, there seems to be some tension between the views that structural models should prescribe exactly what can be done and the view that they should be flexible enough to accommodate a decision maker's needs. For example, we can find in the literature a wide variety of types of intervention that can be considered alongside structural models: beyond the standard ``perfect interventions'' \citep[ch. ~1]{pearl_causality:_2009,hauser_characterization_2012} we have soft interventions \citep{correa_calculus_2020,eberhardt_interventions_2007}, general or fat-hand interventions \citep{eberhardt_interventions_2007,yang_characterizing_2018,glymour_evaluating_2017} and general interventions with unknown targets \citep{brouillard_differentiable_2020}. Offering such a variety of different kinds of interventions seems to acknowledge that decision makers need some flexibility to specify structural models that will suit their needs.

On the other hand, evaulation of causal discovery research almost invariably employs a measure that does not depend on any set of options under consideration at all -- as in the structural hamming distance, which counts the number of edges that differ between an inferred structure and a putative ``true'' structure -- or that assumes that options are given by perfect interventions with respect to the true structure, as in the structural intervention distance \citep{peters_structural_2015}. To offer just a few examples, \citet{brouillard_differentiable_2020,scherrer_learning_2022,toth_active_2022,forre_constraint-based_2018,chickering_optimal_2003,ng_graph_2019,zheng_dags_2018,spirtes_causation_1993} all evaluate their methods according to one or both of these kinds of measures.

Why do we have on the one hand an acknowledgement of the need to allow ``interventions'' in structural models to be flexible enough to accommodate a decision maker's needs, while on the other hand causal discovery methods are evaluated only according to a rigid interpretation of perfect interventions? One of the reasons for this, we presume, is that if we consider a very broad class of interventions -- say, general interventions with unknown targets -- then a structural model places no constraints on what consequences can be achieved by an arbitrary intervention. However, we want causal discovery to tell us something useful, and restricting our attention to what it tells us about perfect interventions ensures that at least it tells us something nontrivial. Exactly when this is also something useful, we don't know.

In short, there is some conceptual difficulty for structural models in determining how they should interface with a given decision maker's options. In this thesis, we ask (loosely speaking) the reverse of this question; rather than starting with a structural model and asking how we can accommodate a decision maker, we start with a decision maker and ask what sorts of models they might want to use.

\section{Outlining our approach}

As we have noted, decision making requires the consideration of a collection of hypotheticals -- specifically, a decision maker must consider the options she has available, and specifically wants to consider the consequences of choosing each of these options. Our approach is to suppose that our job is to help a decision maker evaluate their options. This is an idealisation; a lot of causal analysis doesn't end up directly making a decision, but it might help a third party's decisions in ways the analyst may or may not anticipate. However, it's a different idealisation to the frameworks discussed above. Potential outcomes depend on certain counterfactual statements, structural models depend on ideal interventions and our approach depends on a decision maker's set of options. However, decision making is often at least indirectly the aim of causal inference, and decision problems require the decision maker to consider a set of options, while they do not require a decision to consider counterfactuals or interventions. 

This approach, which can be called a \emph{decision theoretic approach to casual inference}, has previously been explored by \citet{heckerman_decision-theoretic_1995} as well as \citet{dawid_causal_2000,dawid_influence_2002,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. Our approach builds on this earlier work, with a particular focus on the way assumptions of symmetry or regularity in sequential decision problems can lead to models that support non-trivial inferences from data.

To a decision maker, our approach offers the possibility of analysing their problem with fewer assumptions. The two approaches surveyed above require a decision maker who wishes to formally pose their problem to accept a set of options to consider, probability theory, an account of causal effects (whether structural or some other kind of counterfactual) and some ``bridging'' assumption that links their options to the causal effects. In contrast, we only require them to accept a set of options and probability theory. In either case, the decision maker will also have to make additional assumptions that reflect their best guesses about how to use their available data to make a good choice. A key question is whether this approach facilitates solutions to practical decision making problems with assumptions that differ from those that must be made under the existing widely used frameworks.

A basic condition that corresponds approximately to unconfoundedness in standard causal analysis is the assumption of \emph{conditionally independent and identical responses}. In the spirit of De Finetti's analysis of conditionally independent and identically distributed sequences, we examine in Chapter \ref{ch:evaluating_decisions} the relationship between conditionally independent and identical responses and symmetries of a decision making model. This offers an alternative means of analysing assumptions of unconfoundedness in terms of the interchangeability of different datasets. The basic idea here is, instead of making assumptions about which structural causal relationships hold or how counterfactual quantities are distributed, we phrase assumptions in terms of which experiments are essentially identical. For example, the assumption that the \emph{response functions} in a model of a combined sequence of observational and experimental data are identical, we show, is equivalent to the assumption that collecting only the observational data is essentially identical to collecting only the experimental data. We regard this as a mostly negative result; for most decision problems, a precise assumption of identical response functions is usually untenable, though it may be tenable in some approximate form.

The established approaches to causal inference have received a great deal of cumulative development effort, and many specific applications have already been studied. In Chapter \ref{ch:other_causal_frameworks}, we show how to represent Causal Bayesian Networks (a kind of structural causal model) and Potential Outcomes models as decision making models. In particular, we show how each framework by default leaves complementary pieces of a model underspecified. Causal Bayesian Networks are, by default, non-sequential and so one must specify how they can be ``unrolled'' into a sequence model in order to use them to model a decision problem. Potential outcomes models are sequential, but do not provide an unambiguous way to include the options available to the decision maker into the model. Because we can perform this translation, this means that it's possible in principle to translate established application specific reasoning to our framework.

The results from Chapter \ref{ch:evaluating_decisions} are mostly negative in their practical relevance -- certain assumptions that would make inference from data possible are usually untenable. In Chapter \ref{ch:other_causal_frameworks} we explore the weaker assumption of \emph{precedent} (``whatever I can do, it's been done by somebody before''). We prove Theorem \ref{th:det_obs_to_cons}, which establishes that, subject to the additional assumption of \emph{generic probabilistic relations between conditionals}, one can conclude from a conditional independence in observed data that there is a corresponding invariant relationship among the consequences of every option available to the decision maker. 

In the structural models literature generic probabilistic relations between conditionals have been used as an assumption to justify the inference of directed causal relationships. A classic result from \citet{meek_strong_1995} establishes that an assumption of generic probabilistic relations between conditionals ``in the causal direction'' can support the assumption of \emph{faithfulness}, which facilitates the inference of structural models from conditional independences in observed data. An observation subsequently made by a number of authors, for example \citet{lemeire_replacing_2013}, is that these kinds of generic relationships often intuitively hold in the causal direction, and can be violated in the countercausal direction. We use the same idea of generic relationships between conditionals, but by making the assumption of precedent we can skip many steps of reasoning. To use the assumption of generic relationships between conditionals to infer the consequences of choosing different options, a structural modeller needs to infer the causal structure, assume sufficiency (or some other assumption strong enough to support structural identifiability) and then make additional assumptions to connect the intervention operations in the structural model back to the actual options under consideration. With our result, a decision maker can make a similar assumption of generic relationships between conditionals, assume that the consequences of their choices have precedent, and immediately conclude from an observed conditional independence that an observed relationship is invariant over their actual options at hand. Furthermore, we argue in Section \ref{sec:precedent} that an assumption similar to the assumption of precedent is very often built into structural models.

In Theorem \ref{th:det_obs_to_cons}, we assume generic relationships between the key conditional distributions. There is a large literature on methods to identify ``generic relationships'' between conditional probabilities in order to infer causal directions. An open question is whether these methods provide evidence the right kind of ``genericity'' for the application of Theorem \ref{th:det_obs_to_cons}.

In Chapter \ref{ch:other_causal_frameworks} we also examine additional justifications of the assumption of independent and identical response functions. We show how, in addition to the criterion of equivalent prediction problems presented in Chapter \ref{ch:evaluating_decisions}, this assumption can also in some circumstances be justified by an assumption of equivalent options, or by considering certain variables that determine the manner in which outputs respond to inputs to be ``pseudo-observable'' and impose constraints that we might impose on observed variables that play a similar role.

Chapters \ref{ch:tech_prereq} and \ref{ch:2p_statmodels} provide context and prerequisites for the work that follows. In particular, causal inference theories aren't the only theories that address decision making algorithms. These questions are also addressed by the fields of reinforcement learning, optimal control and statistical decision theory (and, no doubt, others besides). One distinction we can draw between these fields and the field of causal inference is that a key difficulty in causal inference problems is just how to relate consequences of actions to observations. In reinforcement learning, an \emph{environment} is typically assumed that represents the ``ground truth'' of consequences of actions, and the history of consequences can be used to infer which environment an agent is operating in \citep{barto_reinforcement_1998}. While optimal control is such a large field it's inappropriate to make any sweeping generalisations, basic versions of control theory assume a \emph{system model} is available that maps states and inputs to updated states and outputs \citep{ogata_discrete-time_1995}. Finally, in statistical decision theory the relevant notion of consequences of actions is given by the \emph{state} and the \emph{loss}, which like the environment in reinforcement learning, are basic elements of the problem \citep{wald_statistical_1950}.

There is substantial overlap between these different methods for relating observations to consequences. For example, \citet[Chap.~4]{lattimore_learning_2017} shows how the environment model in a reinforcement learning problem can be specified using a causal graphical model. In Chapter \ref{ch:2p_statmodels} we survey the literature on modelling decision problems, and show that many different decision theories posit that decision models, including \emph{evidential decision theory} and \emph{causal decision theory} share the same basic type. We also show how a particular class of decision making models -- a class that contains the models investigated in all subsequent chapters -- induce classical statistical decision problems.

The mathematical basis for essentially all of the work in this thesis is probability theory, though we make use of nonstandard constructions within the theory to facilitate the representation of sets of options in the models we consider. We introduce the relevant theory in Chapter \ref{ch:tech_prereq}. As is common in causal inference, we often use a graphical language to represent probabilistic decision models. The language we use is somewhat different to the directed graphs that are standard in the area. We use a string diagram notation that can be related to ordinary directed graphs as in \citep{fong_causal_2013}, but which also supports equality statements and a collection of transformations that can be applied to a diagram to yield an equivalent diagram.

We conclude in Chapter \ref{ch:discussion}, revisiting key results from this thesis and surveying important questions that they raise.
